<div align="center">
  <img src="img/master.png" alt="Robot Image">
  <h1>The LLM Deep Dive</h1>
  <p align="center">
    ğŸ¦ <a href="https://twitter.com/charoori_ai">Follow me on Twitter</a> â€¢
    ğŸ“§ <a href="mailto:chandrahas.aroori@gmail.com?subject=LLM%20Cookbook">Contact on Email</a>
  </p>
</div>
<br/>

In an age of GPT, I'm going to handwrite the best links I've used to learn LLMs.

**Welcome.**

PS: This is for people trying to go deeper. If you want something kind of basic, look elsewhere.

#### How to use this guide?
Start with Easy links in each section. 
Each area has multiple types of subtopics each of which will go more in depth. In the event there are no articles, feel free to email for additions or raise a PR.

#### Table of contents
- [This section talks about various aspects of the Agentic LLMs](#this-section-talks-about-various-aspects-of-the-agentic-llms)
  - [ğŸŸ© Methodology](#-methodology)
    - [Distillation](#distillation)
  - [ğŸŸ© Datasets](#-datasets)
  - [ğŸŸ© Pipeline](#-pipeline)
    - [Training](#training)
    - [Inference](#inference)
      - [RAG](#rag)
    - [Prompting](#prompting)
  - [ğŸŸ© FineTuning](#-finetuning)
    - [Quantized FineTuning](#quantized-finetuning)
    - [DPT](#dpt)
    - [ORPO](#orpo)
  - [ğŸŸ© Quantization](#-quantization)
    - [Post Training Quantization](#post-training-quantization)
      - [Static/Dynamic Quantization](#staticdynamic-quantization)
      - [GPTQ](#gptq)
      - [GGUF](#gguf)
      - [LLM.int8()](#llmint8)
    - [Quantization Aware Training â†’ 1BIT LLM](#quantization-aware-training--1bit-llm)
  - [ğŸŸ© RL in LLM](#-rl-in-llm)
  - [ğŸŸ© Coding](#-coding)
    - [Torch Fundamentals](#torch-fundamentals)
  - [ğŸŸ© Engineering](#-engineering)
    - [Flash Attention 2](#flash-attention-2)
    - [KV Cache](#kv-cache)
    - [Inference â†’ Batched?](#inference--batched)
    - [Python Advanced](#python-advanced)
      - [Decorators](#decorators)
      - [Context Managers](#context-managers)
    - [Triton Kernels](#triton-kernels)
    - [CuDA](#cuda)
    - [JAX / XLA JIT compilers](#jax--xla-jit-compilers)
    - [Model Exporting (vLLM, Llama.cpp, QLoRA)](#model-exporting-vllm-llamacpp-qlora)
    - [ML Debugging](#ml-debugging)
  - [ğŸŸ© Benchmarks](#-benchmarks)
  - [ğŸŸ© Modifications](#-modifications)
    - [Model Merging](#model-merging)
      - [Linear Mapping](#linear-mapping)
      - [SLERP](#slerp)
      - [TIES](#ties)
      - [DARE](#dare)
    - [MoE](#moe)
  - [ğŸŸ© Misc Algorithms](#-misc-algorithms)
    - [Chained Matrix Unit](#chained-matrix-unit)
    - [Gradient Checkpointing](#gradient-checkpointing)
    - [Chunked Cross Entropy](#chunked-cross-entropy)
    - [BPE](#bpe)
  - [ğŸŸ© Explainability](#-explainability)
    - [Sparse Autoencoders](#sparse-autoencoders)
    - [Task Vectors](#task-vectors)
    - [Counterfactuals](#counterfactuals)
  - [ğŸŸ© MultiModal Transformers](#-multimodal-transformers)
    - [Audio](#audio)
      - [Whisper Models](#whisper-models)
      - [Diarization](#diarization)
  - [ğŸŸ© Adversarial methods](#-adversarial-methods)
  - [ğŸŸ© Misc](#-misc)
  - [ğŸŸ© Add to the guide:](#-add-to-the-guide)




### ğŸŸ© Model Architecture
This section talks about the key aspects of LLM architecture.
> ğŸ“ Try to cover basics of Transformers, then understand the GPT architecture before diving deeper into other concepts
#### Transformer Architecture
- [Jay Alamar - Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) ![Easy](https://img.shields.io/badge/difficulty-Easy-green)
##### Tokenization
##### Positional Encoding
###### Rotational Positional Encoding
###### Rotary Positional Encoding

#### GPT Architecture

#### Architecture Llama


#### Attention

#### Loss
##### Cross-Entropy Loss

---
### ğŸŸ© Agentic LLMs
This section talks about various aspects of the Agentic LLMs
---
### ğŸŸ© Methodology
This section tries to cover various methodologies used in LLMs. 
#### Distillation

---
### ğŸŸ© Datasets

---
### ğŸŸ© Pipeline
#### Training
#### Inference
##### RAG
#### Prompting

---
### ğŸŸ© FineTuning
#### Quantized FineTuning
#### DPT
#### ORPO

---
### ğŸŸ© Quantization
#### Post Training Quantization
##### Static/Dynamic Quantization
##### GPTQ
##### GGUF
##### LLM.int8()
#### Quantization Aware Training â†’ 1BIT LLM

---
### ğŸŸ© RL in LLM

---
### ğŸŸ© Coding
#### Torch Fundamentals

---
### ğŸŸ© Engineering
#### Flash Attention 2
#### KV Cache
#### Inference â†’ Batched?
#### Python Advanced
##### Decorators
##### Context Managers
#### Triton Kernels
#### CuDA
#### JAX / XLA JIT compilers
#### Model Exporting (vLLM, Llama.cpp, QLoRA)
#### ML Debugging

---
### ğŸŸ© Benchmarks

---
### ğŸŸ© Modifications
#### Model Merging
##### Linear Mapping
##### SLERP
##### TIES
##### DARE
#### MoE

---
### ğŸŸ© Misc Algorithms
#### Chained Matrix Unit
#### Gradient Checkpointing
#### Chunked Cross Entropy
#### BPE

---
### ğŸŸ© Explainability
#### Sparse Autoencoders
- [Sparse AutoEncoders Explained](https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html) ![Easy](https://img.shields.io/badge/difficulty-Easy-green)
#### Task Vectors
#### Counterfactuals

---
### ğŸŸ© MultiModal Transformers
#### Audio
##### Whisper Models
##### Diarization

---
### ğŸŸ© Adversarial methods

---

### ğŸŸ© Misc
- [Tweet on what to learn in ML (RT by Karpathy)](https://x.com/youraimarketer/status/1778992208697258152) ![Hard](https://img.shields.io/badge/difficulty-Hard-red)
---


### ğŸŸ© Add to the guide:
Add links you find useful through pull requests. 
<!-- Use the following code for sample links:
- [Link 1](http://example.com) ![Hard](https://img.shields.io/badge/difficulty-Hard-red)
- [Link 2](http://example.com) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
- [Link 3](http://example.com) ![Easy](https://img.shields.io/badge/difficulty-Easy-green) -->