<div align="center">
  <img src="img/master.png" alt="Robot Image">
  <h1>The LLM Deep Dive</h1>
  <p align="center">
    üê¶ <a href="https://twitter.com/charoori_ai">Follow me on Twitter</a> ‚Ä¢
    üìß <a href="mailto:chandrahas.aroori@gmail.com?subject=LLM%20Cookbook">Contact on Email</a>
  </p>
</div>
<br/>

In an age of GPT, I'm going to handwrite the best links I've used to learn LLMs.

**Welcome.**

PS: This is for people trying to go deeper. If you want something kind of basic, look elsewhere.

#### ‚óªÔ∏èHow to use this guide?
Start by going through the Table of contents. See what you've already read and what you haven't.
Then, start with Easy links in each section. 
Each area has multiple types of subtopics each of which will go more in depth. In the event there are no articles, feel free to email for additions or raise a PR.

#### ‚óªÔ∏èTable of contents
- [üü© Model Architecture](#-model-architecture)
  - [‚óªÔ∏èTransformer Architecture](#Ô∏ètransformer-architecture)
    - [Tokenization](#tokenization)
    - [Positional Encoding](#positional-encoding)
      - [Rotational Positional Encoding](#rotational-positional-encoding)
      - [Rotary Positional Encoding](#rotary-positional-encoding)
  - [‚óªÔ∏èGPT Architecture](#Ô∏ègpt-architecture)
  - [‚óªÔ∏èAttention](#Ô∏èattention)
  - [‚óªÔ∏èLoss](#Ô∏èloss)
    - [Cross-Entropy Loss](#cross-entropy-loss)
- [üü© Agentic LLMs](#-agentic-llms)
- [üü© Methodology](#-methodology)
  - [‚óªÔ∏èDistillation](#Ô∏èdistillation)
- [üü© Datasets](#-datasets)
- [üü© Pipeline](#-pipeline)
  - [‚óªÔ∏èTraining](#Ô∏ètraining)
  - [‚óªÔ∏èInference](#Ô∏èinference)
    - [RAG](#rag)
  - [‚óªÔ∏èPrompting](#Ô∏èprompting)
- [üü© FineTuning](#-finetuning)
  - [‚óªÔ∏èQuantized FineTuning](#Ô∏èquantized-finetuning)
  - [‚óªÔ∏èLoRA](#Ô∏èlora)
  - [‚óªÔ∏èDPO](#Ô∏èdpo)
  - [‚óªÔ∏èORPO](#Ô∏èorpo)
  - [‚óªÔ∏èRLHF](#Ô∏èrlhf)
- [üü© Quantization](#-quantization)
  - [‚óªÔ∏èPost Training Quantization](#Ô∏èpost-training-quantization)
    - [Static/Dynamic Quantization](#staticdynamic-quantization)
    - [GPTQ](#gptq)
    - [GGUF](#gguf)
    - [LLM.int8()](#llmint8)
  - [‚óªÔ∏èQuantization Aware Training ‚Üí 1BIT LLM](#Ô∏èquantization-aware-training--1bit-llm)
- [üü© RL in LLM](#-rl-in-llm)
- [üü© Coding](#-coding)
  - [‚óªÔ∏èTorch Fundamentals](#Ô∏ètorch-fundamentals)
- [üü© Deployment](#-deployment)
- [üü© Engineering](#-engineering)
  - [‚óªÔ∏èFlash Attention 2](#Ô∏èflash-attention-2)
  - [‚óªÔ∏èKV Cache](#Ô∏èkv-cache)
  - [‚óªÔ∏èBatched Inference](#Ô∏èbatched-inference)
  - [‚óªÔ∏èPython Advanced](#Ô∏èpython-advanced)
    - [Decorators](#decorators)
    - [Context Managers](#context-managers)
  - [‚óªÔ∏èTriton Kernels](#Ô∏ètriton-kernels)
  - [‚óªÔ∏èCuDA](#Ô∏ècuda)
  - [‚óªÔ∏èJAX / XLA JIT compilers](#Ô∏èjax--xla-jit-compilers)
  - [‚óªÔ∏èModel Exporting (vLLM, Llama.cpp, QLoRA)](#Ô∏èmodel-exporting-vllm-llamacpp-qlora)
  - [‚óªÔ∏èML Debugging](#Ô∏èml-debugging)
- [üü© Benchmarks](#-benchmarks)
- [üü© Modifications](#-modifications)
  - [‚óªÔ∏èModel Merging](#Ô∏èmodel-merging)
    - [Linear Mapping](#linear-mapping)
    - [SLERP](#slerp)
    - [TIES](#ties)
    - [DARE](#dare)
  - [‚óªÔ∏èMoE](#Ô∏èmoe)
- [üü© Misc Algorithms](#-misc-algorithms)
  - [‚óªÔ∏èChained Matrix Unit](#Ô∏èchained-matrix-unit)
  - [‚óªÔ∏èGradient Checkpointing](#Ô∏ègradient-checkpointing)
  - [‚óªÔ∏èChunked Cross Entropy](#Ô∏èchunked-cross-entropy)
  - [‚óªÔ∏èBPE](#Ô∏èbpe)
- [üü© Explainability](#-explainability)
  - [‚óªÔ∏èSparse Autoencoders](#Ô∏èsparse-autoencoders)
  - [‚óªÔ∏èTask Vectors](#Ô∏ètask-vectors)
  - [‚óªÔ∏èCounterfactuals](#Ô∏ècounterfactuals)
- [üü© MultiModal Transformers](#-multimodal-transformers)
  - [‚óªÔ∏èAudio](#Ô∏èaudio)
    - [Whisper Models](#whisper-models)
    - [Diarization](#diarization)
- [üü© Adversarial methods](#-adversarial-methods)
- [üü© Misc](#-misc)
- [üü© Add to the guide:](#-add-to-the-guide)




### üü© Model Architecture
This section talks about the key aspects of LLM architecture.
> üìù Try to cover basics of Transformers, then understand the GPT architecture before diving deeper into other concepts
#### ‚óªÔ∏èTransformer Architecture
- [Jay Alamar - Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) ![Easy](https://img.shields.io/badge/difficulty-Easy-green)
- [Umar Jamil: Attention](https://www.youtube.com/watch?v=bCz4OMemCcA&) ![Easy](https://img.shields.io/badge/difficulty-Easy-green)

##### Tokenization
##### Positional Encoding
###### Rotational Positional Encoding
###### Rotary Positional Encoding
-[Rotary Positional Encoding Explained](https://medium.com/@ngiengkianyew/understanding-rotary-positional-encoding-40635a4d078e)

#### ‚óªÔ∏èGPT Architecture
- [Jay Alamar - Illustrated GPT2](https://jalammar.github.io/illustrated-gpt2/) ![Easy](https://img.shields.io/badge/difficulty-Easy-green)
- [Reproducing GPT-2 (124M) in llm.c in 90 minutes for $20](https://github.com/karpathy/llm.c/discussions/481) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
- [Umar Jamil: Llama Explained](https://www.youtube.com/watch?v=Mn_9W1nCFLo) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
- [Umar Jamil: Llama 2 from Scratch](https://www.youtube.com/watch?v=oM4VmoabDAI) ![Hard](https://img.shields.io/badge/difficulty-Hard-red)

#### ‚óªÔ∏èAttention

#### ‚óªÔ∏èLoss
##### Cross-Entropy Loss

---
### üü© Agentic LLMs
-[Agentic LLMs Deep Dive](https://www.aimon.ai/posts/deep-dive-into-agentic-llm-frameworks)
This section talks about various aspects of the Agentic LLMs

---
### üü© Methodology
This section tries to cover various methodologies used in LLMs. 
#### ‚óªÔ∏èDistillation

---
### üü© Datasets

---
### üü© Pipeline
#### ‚óªÔ∏èTraining
#### ‚óªÔ∏èInference
##### RAG
#### ‚óªÔ∏èPrompting

---
### üü© FineTuning
#### ‚óªÔ∏èQuantized FineTuning
- [Umar Jamil: Quantization](https://www.youtube.com/watch?v=0VdNflU08yA) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
#### ‚óªÔ∏èLoRA
- [Umar Jamil: LoRA Explained](https://www.youtube.com/watch?v=PXWYUTMt-AU) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
#### ‚óªÔ∏èDPO
- [Umar Jamil: DPO Explained](https://www.youtube.com/watch?v=hvGa5Mba4c8) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
#### ‚óªÔ∏èORPO
#### ‚óªÔ∏èRLHF
- [Umar Jamil: RLHF Explained](https://www.youtube.com/watch?v=qGyFrqc34yc) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)

---
### üü© Quantization
#### ‚óªÔ∏èPost Training Quantization
##### Static/Dynamic Quantization
##### GPTQ
##### GGUF
##### LLM.int8()
#### ‚óªÔ∏èQuantization Aware Training ‚Üí 1BIT LLM

---
### üü© RL in LLM

---
### üü© Coding
#### ‚óªÔ∏èTorch Fundamentals

---
### üü© Deployment
- [Achieve 23x LLM Inference Throughput & Reduce p50 Latency](https://charoori.notion.site/Achieve-23x-LLM-Inference-Throughput-Reduce-p50-Latency-17d311b8ed1e818daf2bf1287647e99f)![Hard](https://img.shields.io/badge/difficulty-Hard-red)
- [LLM Inference Optimizations ‚Äî Continuous Batching (Dynamic Batching) and Selective Batching, Orca | by Don Moon | Byte-Sized AI | Medium](https://charoori.notion.site/LLM-Inference-Optimizations-Continuous-Batching-Dynamic-Batching-and-Selective-Batching-Orca--17d311b8ed1e81718f56f7a26ec8f5b8)![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
- [LLM Inference Optimizations - Chunked Prefills and Decode Maximal Batching | by Don Moon | Byte-Sized AI](https://charoori.notion.site/LLM-Inference-Optimizations-Chunked-Prefills-and-Decode-Maximal-Batching-by-Don-Moon-Byte-Size-17d311b8ed1e81d49ec0fedbd351821b)![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
- [LLM Inference Series: 2. The two-phase process behind LLMs' responses | by Pierre Lienhart | Medium](https://charoori.notion.site/LLM-Inference-Series-2-The-two-phase-process-behind-LLMs-responses-by-Pierre-Lienhart-Medium-17d311b8ed1e812d9a80e14442d55eb1)![Hard](https://img.shields.io/badge/difficulty-Hard-red)
- [LLM Inference Series: 4. KV caching, a deeper look | by Pierre Lienhart | Medium](https://charoori.notion.site/LLM-Inference-Series-4-KV-caching-a-deeper-look-by-Pierre-Lienhart-Medium-17d311b8ed1e81a499d7f1cd07cd97c8)![Hard](https://img.shields.io/badge/difficulty-Hard-red)



---
### üü© Engineering
- [ML Engineering; Used for training BLOOM](https://github.com/stas00/ml-engineering/tree/master) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)

- [Low Level Technicals of LLMs](https://www.youtube.com/watch?v=pRM_P6UfdIc) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
- [Fixing bugs in Llama, Mistral, Gemma](https://www.youtube.com/watch?v=TKmfBnW0mQA) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
- [PyTorch Conference Mini Talk](https://www.youtube.com/watch?v=PdtKkc5jB4g) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
- [PyTorch Engineers Meeting Talk](https://www.youtube.com/watch?v=MQwryfkydc0) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
- [Hugging Face Collab Blog](https://huggingface.co/blog/unsloth-trl) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
#### ‚óªÔ∏èFlash Attention 2
#### ‚óªÔ∏èKV Cache
#### ‚óªÔ∏èBatched Inference
#### ‚óªÔ∏èPython Advanced
##### Decorators
##### Context Managers
#### ‚óªÔ∏èTriton Kernels
#### ‚óªÔ∏èCuDA
- [CUDA / GPU Mode lecture Talk](https://www.youtube.com/watch?v=hfb_AIhDYnA) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
#### ‚óªÔ∏èJAX / XLA JIT compilers
#### ‚óªÔ∏èModel Exporting (vLLM, Llama.cpp, QLoRA)
#### ‚óªÔ∏èML Debugging

---
### üü© Benchmarks

---
### üü© Modifications
#### ‚óªÔ∏èModel Merging
-[An Introduction to Model Merging for LLMs](https://developer.nvidia.com/blog/an-introduction-to-model-merging-for-llms/)![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
##### Linear Mapping
##### SLERP
-[Merging tokens to accelerate LLM inference with SLERP](https://medium.com/towards-data-science/merging-tokens-to-accelerate-llm-inference-with-slerp-38a32bf7f194)![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
##### TIES
##### DARE
#### ‚óªÔ∏èMoE

---
### üü© Misc Algorithms
#### ‚óªÔ∏èChained Matrix Unit
#### ‚óªÔ∏èGradient Checkpointing
#### ‚óªÔ∏èChunked Cross Entropy
#### ‚óªÔ∏èBPE

---
### üü© Explainability
#### ‚óªÔ∏èSparse Autoencoders
- [Sparse AutoEncoders Explained](https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html) ![Easy](https://img.shields.io/badge/difficulty-Easy-green)
#### ‚óªÔ∏èTask Vectors
#### ‚óªÔ∏èCounterfactuals

---
### üü© MultiModal Transformers
#### ‚óªÔ∏èAudio
##### Whisper Models
- [Whisper Model Explained](https://www.notta.ai/en/blog/how-to-use-whisper)
##### Diarization

---
### üü© Adversarial methods

---

### üü© Misc
- [Tweet on what to learn in ML (RT by Karpathy)](https://x.com/youraimarketer/status/1778992208697258152) ![Hard](https://img.shields.io/badge/difficulty-Hard-red)
---


### üü© Add to the guide:
Add links you find useful through pull requests. 
<!-- Use the following code for sample links:
- [Link 1](http://example.com) ![Hard](https://img.shields.io/badge/difficulty-Hard-red)
- [Link 2](http://example.com) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
- [Link 3](http://example.com) ![Easy](https://img.shields.io/badge/difficulty-Easy-green) -->
