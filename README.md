<div align="center">
  <img src="img/master.png" alt="Robot Image">
  <h1>The LLM Deep Dive</h1>
  <p align="center">
    ğŸ¦ <a href="https://twitter.com/charoori_ai">Follow me on Twitter</a> â€¢
    ğŸ“§ <a href="mailto:chandrahas.aroori@gmail.com?subject=LLM%20Cookbook">Contact on Email</a>
  </p>
</div>
<br/>

In an age of GPT, I'm going to handwrite the best links I've used to learn LLMs.

**Welcome.**

PS: This is for people trying to go deeper. If you want something kind of basic, look elsewhere.

#### â—»ï¸How to use this guide?
Start by going through the Table of contents. See what you've already read and what you haven't.
Then, start with Easy links in each section. 
Each area has multiple types of subtopics each of which will go more in depth. In the event there are no articles, feel free to email for additions or raise a PR.

#### â—»ï¸Table of contents
- [ğŸŸ© Model Architecture](#-model-architecture)
  - [â—»ï¸Transformer Architecture](#ï¸transformer-architecture)
    - [Tokenization](#tokenization)
    - [Positional Encoding](#positional-encoding)
      - [Rotational Positional Encoding](#rotational-positional-encoding)
      - [Rotary Positional Encoding](#rotary-positional-encoding)
  - [â—»ï¸GPT Architecture](#ï¸gpt-architecture)
  - [â—»ï¸Attention](#ï¸attention)
  - [â—»ï¸Loss](#ï¸loss)
    - [Cross-Entropy Loss](#cross-entropy-loss)
- [ğŸŸ© Agentic LLMs](#-agentic-llms)
- [ğŸŸ© Methodology](#-methodology)
  - [â—»ï¸Distillation](#ï¸distillation)
- [ğŸŸ© Datasets](#-datasets)
- [ğŸŸ© Pipeline](#-pipeline)
  - [â—»ï¸Training](#ï¸training)
  - [â—»ï¸Inference](#ï¸inference)
    - [RAG](#rag)
  - [â—»ï¸Prompting](#ï¸prompting)
- [ğŸŸ© FineTuning](#-finetuning)
  - [â—»ï¸Quantized FineTuning](#ï¸quantized-finetuning)
  - [â—»ï¸LoRA](#ï¸lora)
  - [â—»ï¸DPO](#ï¸dpo)
  - [â—»ï¸ORPO](#ï¸orpo)
  - [â—»ï¸RLHF](#ï¸rlhf)
- [ğŸŸ© Quantization](#-quantization)
  - [â—»ï¸Post Training Quantization](#ï¸post-training-quantization)
    - [Static/Dynamic Quantization](#staticdynamic-quantization)
    - [GPTQ](#gptq)
    - [GGUF](#gguf)
    - [LLM.int8()](#llmint8)
  - [â—»ï¸Quantization Aware Training â†’ 1BIT LLM](#ï¸quantization-aware-training--1bit-llm)
- [ğŸŸ© RL in LLM](#-rl-in-llm)
- [ğŸŸ© Coding](#-coding)
  - [â—»ï¸Torch Fundamentals](#ï¸torch-fundamentals)
- [ğŸŸ© Deployment](#-deployment)
- [ğŸŸ© Engineering](#-engineering)
  - [â—»ï¸Flash Attention 2](#ï¸flash-attention-2)
  - [â—»ï¸KV Cache](#ï¸kv-cache)
  - [â—»ï¸Batched Inference](#ï¸batched-inference)
  - [â—»ï¸Python Advanced](#ï¸python-advanced)
    - [Decorators](#decorators)
    - [Context Managers](#context-managers)
  - [â—»ï¸Triton Kernels](#ï¸triton-kernels)
  - [â—»ï¸CuDA](#ï¸cuda)
  - [â—»ï¸JAX / XLA JIT compilers](#ï¸jax--xla-jit-compilers)
  - [â—»ï¸Model Exporting (vLLM, Llama.cpp, QLoRA)](#ï¸model-exporting-vllm-llamacpp-qlora)
  - [â—»ï¸ML Debugging](#ï¸ml-debugging)
- [ğŸŸ© Benchmarks](#-benchmarks)
- [ğŸŸ© Modifications](#-modifications)
  - [â—»ï¸Model Merging](#ï¸model-merging)
    - [Linear Mapping](#linear-mapping)
    - [SLERP](#slerp)
    - [TIES](#ties)
    - [DARE](#dare)
  - [â—»ï¸MoE](#ï¸moe)
- [ğŸŸ© Misc Algorithms](#-misc-algorithms)
  - [â—»ï¸Chained Matrix Unit](#ï¸chained-matrix-unit)
  - [â—»ï¸Gradient Checkpointing](#ï¸gradient-checkpointing)
  - [â—»ï¸Chunked Cross Entropy](#ï¸chunked-cross-entropy)
  - [â—»ï¸BPE](#ï¸bpe)
- [ğŸŸ© Explainability](#-explainability)
  - [â—»ï¸Sparse Autoencoders](#ï¸sparse-autoencoders)
  - [â—»ï¸Task Vectors](#ï¸task-vectors)
  - [â—»ï¸Counterfactuals](#ï¸counterfactuals)
- [ğŸŸ© MultiModal Transformers](#-multimodal-transformers)
  - [â—»ï¸Audio](#ï¸audio)
    - [Whisper Models](#whisper-models)
    - [Diarization](#diarization)
- [ğŸŸ© Adversarial methods](#-adversarial-methods)
- [ğŸŸ© Misc](#-misc)
- [ğŸŸ© Add to the guide:](#-add-to-the-guide)




### ğŸŸ© Model Architecture
This section talks about the key aspects of LLM architecture.
> ğŸ“ Try to cover basics of Transformers, then understand the GPT architecture before diving deeper into other concepts
#### â—»ï¸Transformer Architecture
- [Jay Alamar - Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) ![Easy](https://img.shields.io/badge/difficulty-Easy-green)
- [Umar Jamil: Attention](https://www.youtube.com/watch?v=bCz4OMemCcA&) ![Easy](https://img.shields.io/badge/difficulty-Easy-green)

##### Tokenization
##### Positional Encoding
###### Rotational Positional Encoding
###### Rotary Positional Encoding
-[Rotary Positional Encoding Explained](https://medium.com/@ngiengkianyew/understanding-rotary-positional-encoding-40635a4d078e)

#### â—»ï¸GPT Architecture
- [Jay Alamar - Illustrated GPT2](https://jalammar.github.io/illustrated-gpt2/) ![Easy](https://img.shields.io/badge/difficulty-Easy-green)
- [Reproducing GPT-2 (124M) in llm.c in 90 minutes for $20](https://github.com/karpathy/llm.c/discussions/481) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
- [Umar Jamil: Llama Explained](https://www.youtube.com/watch?v=Mn_9W1nCFLo) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
- [Umar Jamil: Llama 2 from Scratch](https://www.youtube.com/watch?v=oM4VmoabDAI) ![Hard](https://img.shields.io/badge/difficulty-Hard-red)

#### â—»ï¸Attention

#### â—»ï¸Loss
##### Cross-Entropy Loss

---
### ğŸŸ© Agentic LLMs
-[Agentic LLMs Deep Dive](https://www.aimon.ai/posts/deep-dive-into-agentic-llm-frameworks)
This section talks about various aspects of the Agentic LLMs

---
### ğŸŸ© Methodology
This section tries to cover various methodologies used in LLMs. 
#### â—»ï¸Distillation

---
### ğŸŸ© Datasets

---
### ğŸŸ© Pipeline
#### â—»ï¸Training
#### â—»ï¸Inference
##### RAG
#### â—»ï¸Prompting

---
### ğŸŸ© FineTuning
#### â—»ï¸Quantized FineTuning
- [Umar Jamil: Quantization](https://www.youtube.com/watch?v=0VdNflU08yA) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
#### â—»ï¸LoRA
- [Umar Jamil: LoRA Explained](https://www.youtube.com/watch?v=PXWYUTMt-AU) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
#### â—»ï¸DPO
- [Umar Jamil: DPO Explained](https://www.youtube.com/watch?v=hvGa5Mba4c8) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
#### â—»ï¸ORPO
#### â—»ï¸RLHF
- [Umar Jamil: RLHF Explained](https://www.youtube.com/watch?v=qGyFrqc34yc) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)

---
### ğŸŸ© Quantization
#### â—»ï¸Post Training Quantization
##### Static/Dynamic Quantization
##### GPTQ
##### GGUF
##### LLM.int8()
#### â—»ï¸Quantization Aware Training â†’ 1BIT LLM

---
### ğŸŸ© RL in LLM

---
### ğŸŸ© Coding
#### â—»ï¸Torch Fundamentals

---
### ğŸŸ© Deployment
- [Achieve 23x LLM Inference Throughput & Reduce p50 Latency](https://charoori.notion.site/Achieve-23x-LLM-Inference-Throughput-Reduce-p50-Latency-17d311b8ed1e818daf2bf1287647e99f)![Hard](https://img.shields.io/badge/difficulty-Hard-red)
- [LLM Inference Optimizations â€” Continuous Batching (Dynamic Batching) and Selective Batching, Orca | by Don Moon | Byte-Sized AI | Medium](https://charoori.notion.site/LLM-Inference-Optimizations-Continuous-Batching-Dynamic-Batching-and-Selective-Batching-Orca--17d311b8ed1e81718f56f7a26ec8f5b8)![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
- [LLM Inference Optimizations - Chunked Prefills and Decode Maximal Batching | by Don Moon | Byte-Sized AI](https://charoori.notion.site/LLM-Inference-Optimizations-Chunked-Prefills-and-Decode-Maximal-Batching-by-Don-Moon-Byte-Size-17d311b8ed1e81d49ec0fedbd351821b)![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
- [LLM Inference Series: 2. The two-phase process behind LLMs' responses | by Pierre Lienhart | Medium](https://charoori.notion.site/LLM-Inference-Series-2-The-two-phase-process-behind-LLMs-responses-by-Pierre-Lienhart-Medium-17d311b8ed1e812d9a80e14442d55eb1)![Hard](https://img.shields.io/badge/difficulty-Hard-red)
- [LLM Inference Series: 4. KV caching, a deeper look | by Pierre Lienhart | Medium](https://charoori.notion.site/LLM-Inference-Series-4-KV-caching-a-deeper-look-by-Pierre-Lienhart-Medium-17d311b8ed1e81a499d7f1cd07cd97c8)![Hard](https://img.shields.io/badge/difficulty-Hard-red)



---
### ğŸŸ© Engineering
- [ML Engineering; Used for training BLOOM](https://github.com/stas00/ml-engineering/tree/master) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)

- [Low Level Technicals of LLMs](https://www.youtube.com/watch?v=pRM_P6UfdIc) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
- [Fixing bugs in Llama, Mistral, Gemma](https://www.youtube.com/watch?v=TKmfBnW0mQA) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
- [PyTorch Conference Mini Talk](https://www.youtube.com/watch?v=PdtKkc5jB4g) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
- [PyTorch Engineers Meeting Talk](https://www.youtube.com/watch?v=MQwryfkydc0) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
- [Hugging Face Collab Blog](https://huggingface.co/blog/unsloth-trl) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
#### â—»ï¸Flash Attention 2
#### â—»ï¸KV Cache
#### â—»ï¸Batched Inference
#### â—»ï¸Python Advanced
##### Decorators
##### Context Managers
#### â—»ï¸Triton Kernels
#### â—»ï¸CuDA
- [CUDA / GPU Mode lecture Talk](https://www.youtube.com/watch?v=hfb_AIhDYnA) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
#### â—»ï¸JAX / XLA JIT compilers
#### â—»ï¸Model Exporting (vLLM, Llama.cpp, QLoRA)
#### â—»ï¸ML Debugging

---
### ğŸŸ© Benchmarks

---
### ğŸŸ© Modifications
#### â—»ï¸Model Merging
##### Linear Mapping
##### SLERP
##### TIES
##### DARE
#### â—»ï¸MoE

---
### ğŸŸ© Misc Algorithms
#### â—»ï¸Chained Matrix Unit
#### â—»ï¸Gradient Checkpointing
#### â—»ï¸Chunked Cross Entropy
#### â—»ï¸BPE

---
### ğŸŸ© Explainability
#### â—»ï¸Sparse Autoencoders
- [Sparse AutoEncoders Explained](https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html) ![Easy](https://img.shields.io/badge/difficulty-Easy-green)
#### â—»ï¸Task Vectors
#### â—»ï¸Counterfactuals

---
### ğŸŸ© MultiModal Transformers
#### â—»ï¸Audio
##### Whisper Models
- [Whisper Model Explained](https://www.notta.ai/en/blog/how-to-use-whisper)
##### Diarization

---
### ğŸŸ© Adversarial methods

---

### ğŸŸ© Misc
- [Tweet on what to learn in ML (RT by Karpathy)](https://x.com/youraimarketer/status/1778992208697258152) ![Hard](https://img.shields.io/badge/difficulty-Hard-red)
---


### ğŸŸ© Add to the guide:
Add links you find useful through pull requests. 
<!-- Use the following code for sample links:
- [Link 1](http://example.com) ![Hard](https://img.shields.io/badge/difficulty-Hard-red)
- [Link 2](http://example.com) ![Medium](https://img.shields.io/badge/difficulty-Medium-yellow)
- [Link 3](http://example.com) ![Easy](https://img.shields.io/badge/difficulty-Easy-green) -->
